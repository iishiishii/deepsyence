{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import {InferenceSession, Tensor} from 'onnxruntime-node';\n",
    "const ndarray = require('ndarray')\n",
    "const ops = require('ndarray-ops')\n",
    "const fs = require('fs')\n",
    "const bisweb = require('biswebnode')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "Promise { <pending> }\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_1 l {\n",
      "  dims: [ 211, 224, 224, 1 ],\n",
      "  type: 'float32',\n",
      "  data: Float32Array(10587136) [\n",
      "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "    0, 0, 0, 0,\n",
      "    ... 10587036 more items\n",
      "  ],\n",
      "  size: 10587136\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "failed to inference ONNX model: TypeError: Cannot read property 'data' of undefined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "++++\t loaded image from ../model/head.nii.gz. Dim= [ 204, 224, 160, 1, 1 ] RAS spa=1,1,1,0.02500000037252903,1 type=sshort\n"
     ]
    }
   ],
   "source": [
    "import {InferenceSession, Tensor} from 'onnxruntime-node';\n",
    "const ndarray = require('ndarray')\n",
    "const ops = require('ndarray-ops')\n",
    "const fs = require('fs')\n",
    "const bisweb = require('biswebnode')\n",
    "\n",
    "\n",
    "function imageDataToTensor(data, dims){\n",
    "  // 1a. Extract the R, G, and B channels from the data to form a 3D int array\n",
    "  const [R, G, B] = new Array([], [], []);\n",
    "  for (let i = 0; i < data.length; i += 4) {\n",
    "    R.push(data[i]);\n",
    "    G.push(data[i + 1]);\n",
    "    B.push(data[i + 2]);\n",
    "    // 2. skip data[i + 3] thus filtering out the alpha channel\n",
    "  }\n",
    "  ///console.log(R);\n",
    "  //console.log(G);\n",
    "  //console.log(B);\n",
    "  // 1b. concatenate RGB ~= transpose [224, 224, 3] -> [3, 224, 224]\n",
    "  const transposedData = R.concat(G).concat(B);\n",
    "\n",
    "  // 3. convert to float32\n",
    "  let i, l = transposedData.length; // length, we need this for the loop\n",
    "  const float32Data = new Float32Array(dims[0]*dims[1]*dims[2]); // create the Float32Array for output\n",
    "  for (i = 0; i < l; i++) {\n",
    "    float32Data[i] = transposedData[i] ; // convert to float\n",
    "  }\n",
    "\n",
    "  const inputTensor = new Tensor(\"float32\", float32Data, dims);\n",
    "  return inputTensor;\n",
    "}\n",
    "\n",
    "let img=new bisweb.BisWebImage();\n",
    "var data;\n",
    "\n",
    "\n",
    "// use an async context to call onnxruntime functions.\n",
    "async function main() {\n",
    "    try {\n",
    "        // create a new session and load the specific model.\n",
    "        //\n",
    "        // the model in this example contains a single MatMul node\n",
    "        // it has 2 inputs: 'a'(float32, 3x4) and 'b'(float32, 4x3)\n",
    "        // it has 1 output: 'c'(float32, 3x3)\n",
    "        img.load(\"../model/head.nii.gz\").then( () => {\n",
    "          // console.log('Image Loaded = ',img.getImageData(), img.internal.dimensions);\n",
    "          return img.getImageData()\n",
    "        }).catch( (e) => {\n",
    "          console.log(e,e.stack);\n",
    "        });\n",
    "        data = imageDataToTensor(img, [211,224,224,1])\n",
    "        console.log(data.type)\n",
    "\n",
    "        const session = await InferenceSession.create('../model/model.onnx');\n",
    "        console.log(session.inputNames[0], data)\n",
    "\n",
    "        // prepare feeds. use model input names as keys.\n",
    "        const feeds = { input_1:data };\n",
    "\n",
    "        // feed inputs and run\n",
    "        const results = await session.run(feeds);\n",
    "\n",
    "        // read from results\n",
    "        const dataC = results.c.data;\n",
    "        console.log(`data of result tensor 'c': ${dataC}`);\n",
    "\n",
    "    } catch (e) {\n",
    "        console.error(`failed to inference ONNX model: ${e}.`);\n",
    "    }\n",
    "}\n",
    "\n",
    "main();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function imageDataToTensor(data, dims){\n",
    "  // 1a. Extract the R, G, and B channels from the data to form a 3D int array\n",
    "  const [R, G, B] = new Array([], [], []);\n",
    "  for (let i = 0; i < data.length; i += 4) {\n",
    "    R.push(data[i]);\n",
    "    G.push(data[i + 1]);\n",
    "    B.push(data[i + 2]);\n",
    "    // 2. skip data[i + 3] thus filtering out the alpha channel\n",
    "  }\n",
    "  ///console.log(R);\n",
    "  //console.log(G);\n",
    "  //console.log(B);\n",
    "  // 1b. concatenate RGB ~= transpose [224, 224, 3] -> [3, 224, 224]\n",
    "  const transposedData = R.concat(G).concat(B);\n",
    "\n",
    "  // 3. convert to float32\n",
    "  let i, l = transposedData.length; // length, we need this for the loop\n",
    "  const float32Data = new Float32Array(dims[0]*dims[1]*dims[2]); // create the Float32Array for output\n",
    "  for (i = 0; i < l; i++) {\n",
    "    float32Data[i] = transposedData[i] / 255.0; // convert to float\n",
    "  }\n",
    "\n",
    "  const inputTensor = new Tensor(\"float32\", float32Data, dims);\n",
    "  return inputTensor;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TypeError: Only absolute URLs are supported\n",
      "    at getNodeRequestOptions (/home/thuy/repo/deepsyence/node_modules/node-fetch/lib/index.js:1305:9)\n",
      "    at /home/thuy/repo/deepsyence/node_modules/node-fetch/lib/index.js:1410:19\n",
      "    at new Promise (<anonymous>)\n",
      "    at fetch (/home/thuy/repo/deepsyence/node_modules/node-fetch/lib/index.js:1407:9)\n",
      "    at Object.createSessionHandler (/home/thuy/repo/deepsyence/node_modules/onnxruntime-web/dist/ort-web.node.js:6:159912)\n",
      "    at Function.create (/home/thuy/repo/deepsyence/node_modules/onnxruntime-common/dist/ort-common.node.js:6:6607)\n",
      "    at async evalmachine.<anonymous>:5:17\n",
      "    at async Object.execute (/home/thuy/.nvm/versions/node/v14.4.0/lib/node_modules/tslab/dist/executor.js:175:17)\n",
      "    at async JupyterHandlerImpl.handleExecuteImpl (/home/thuy/.nvm/versions/node/v14.4.0/lib/node_modules/tslab/dist/jupyter.js:219:18)\n"
     ]
    }
   ],
   "source": [
    "// create an inference session, using WebGL backend. (default is 'wasm') \n",
    "//const session = await ort.InferenceSession.create('./model/squeezenet1_1.onnx', { executionProviders: ['wasm'] }); \n",
    "const session = await InferenceSession.create('../model/model.onnx', { executionProviders: ['wasm'] });"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TypeError: Only absolute URLs are supported\n",
      "    at getNodeRequestOptions (/home/thuy/repo/deepsyence/node_modules/node-fetch/lib/index.js:1305:9)\n",
      "    at /home/thuy/repo/deepsyence/node_modules/node-fetch/lib/index.js:1410:19\n",
      "    at new Promise (<anonymous>)\n",
      "    at fetch (/home/thuy/repo/deepsyence/node_modules/node-fetch/lib/index.js:1407:9)\n",
      "    at Object.createSessionHandler (/home/thuy/repo/deepsyence/node_modules/onnxruntime-web/dist/ort-web.node.js:6:159912)\n",
      "    at Function.create (/home/thuy/repo/deepsyence/node_modules/onnxruntime-common/dist/ort-common.node.js:6:6607)\n",
      "    at async evalmachine.<anonymous>:5:17\n",
      "    at async Object.execute (/home/thuy/.nvm/versions/node/v14.4.0/lib/node_modules/tslab/dist/executor.js:175:17)\n",
      "    at async JupyterHandlerImpl.handleExecuteImpl (/home/thuy/.nvm/versions/node/v14.4.0/lib/node_modules/tslab/dist/jupyter.js:219:18)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "async function runModel(model, preprocessedData) {\n",
    "  const start = new Date();\n",
    "  try {\n",
    "    const input = preprocessedData;\n",
    "    const feeds = {float_input: input};\n",
    "    // feeds[model.inputNames[0]] = preprocessedData;\n",
    "    const outputData = await model.run(feeds);\n",
    "    const end = new Date();\n",
    "    const inferenceTime = (end.getTime() - start.getTime());\n",
    "    const output = outputData[model.outputNames[0]];\n",
    "    return [output, inferenceTime];\n",
    "  } catch (e) {\n",
    "    console.error(e);\n",
    "    throw new Error();\n",
    "  }\n",
    "}\n",
    "\n",
    "//The softmax transforms values to be between 0 and 1\n",
    "function softmax(resultArray) {\n",
    "  // Get the largest value in the array.\n",
    "  const largestNumber = Math.max(...resultArray);\n",
    "  // Apply exponential function to each result item subtracted by the largest number, use reduce to get the previous result number and the current number to sum all the exponentials results.\n",
    "  const sumOfExp = resultArray.map((resultItem) => Math.exp(resultItem - largestNumber)).reduce((prevNumber, currentNumber) => prevNumber + currentNumber);\n",
    "  //Normalizes the resultArray by dividing by the sum of all exponentials; this normalization ensures that the sum of the components of the output vector is 1.\n",
    "  return resultArray.map((resultValue, index) => {\n",
    "    return Math.exp(resultValue - largestNumber) / sumOfExp;\n",
    "  });\n",
    "}\n",
    "\n",
    "const [res, time] =  await runModel(session, data);\n",
    "var output = res.data;\n",
    "var inferenceTime = time;\n",
    "var results = softmax(Array.prototype.slice.call(output))\n",
    "\n",
    "// var topResults = [];\n",
    "// for (let i = 0; i < results.length; i++) {\n",
    "//   if (results[i] > 0.3) {\n",
    "//     topResults.push([classes[i] + \": \" + results[i]]);\n",
    "//   }\n",
    "// }\n",
    "\n",
    "console.log(inferenceTime);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async function runModel(model, preprocessedData) {\n",
    "    const start = new Date();\n",
    "    try {\n",
    "      const input = new ort.Tensor(new Float32Array(preprocessedData), [1, 380]);\n",
    "      const feeds = {float_input: input};\n",
    "      // feeds[model.inputNames[0]] = preprocessedData;\n",
    "      const outputData = await model.run(feeds);\n",
    "      const end = new Date();\n",
    "      const inferenceTime = (end.getTime() - start.getTime());\n",
    "      const output = outputData[model.outputNames[0]];\n",
    "      return [output, inferenceTime];\n",
    "    } catch (e) {\n",
    "      console.error(e);\n",
    "      throw new Error();\n",
    "    }\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "//The softmax transforms values to be between 0 and 1\n",
    "function softmax(resultArray) {\n",
    "  // Get the largest value in the array.\n",
    "  const largestNumber = Math.max(...resultArray);\n",
    "  // Apply exponential function to each result item subtracted by the largest number, use reduce to get the previous result number and the current number to sum all the exponentials results.\n",
    "  const sumOfExp = resultArray.map((resultItem) => Math.exp(resultItem - largestNumber)).reduce((prevNumber, currentNumber) => prevNumber + currentNumber);\n",
    "  //Normalizes the resultArray by dividing by the sum of all exponentials; this normalization ensures that the sum of the components of the output vector is 1.\n",
    "  return resultArray.map((resultValue, index) => {\n",
    "    return Math.exp(resultValue - largestNumber) / sumOfExp;\n",
    "  });\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1:28 - Cannot find name 'runModel'.\n",
      "1:37 - Cannot find name 'session'.\n",
      "4:15 - Cannot find name 'softmax'.\n"
     ]
    }
   ],
   "source": [
    "const [res, time] =  await runModel(session, data);\n",
    "var output = res.data;\n",
    "var inferenceTime = time;\n",
    "var results = softmax(Array.prototype.slice.call(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var topResults = [];\n",
    "for (let i = 0; i < results.length; i++) {\n",
    "  if (results[i] > 0.3) {\n",
    "    topResults.push([classes[i] + \": \" + results[i]]);\n",
    "  }\n",
    "}\n",
    "\n",
    "console.log(topResults);"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f7c4d44365b28014734406e4d617c1e1f76ea196def854c7b951a230f6e24f1"
  },
  "kernelspec": {
   "display_name": "JavaScript",
   "language": "javascript",
   "name": "jslab"
  },
  "language_info": {
   "file_extension": ".js",
   "mimetype": "text/javascript",
   "name": "javascript",
   "version": ""
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
